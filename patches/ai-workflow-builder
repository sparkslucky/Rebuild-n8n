diff --git a/packages/@n8n/ai-workflow-builder.ee/src/ai-workflow-builder-agent.service.ts b/packages/@n8n/ai-workflow-builder.ee/src/ai-workflow-builder-agent.service.ts
index a716036f30..59b8daf42b 100644
--- a/packages/@n8n/ai-workflow-builder.ee/src/ai-workflow-builder-agent.service.ts
+++ b/packages/@n8n/ai-workflow-builder.ee/src/ai-workflow-builder-agent.service.ts
@@ -1,4 +1,5 @@
 import { ChatAnthropic } from '@langchain/anthropic';
+import type { BaseChatModel } from '@langchain/core/language_models/chat_models';
 import { AIMessage, ToolMessage } from '@langchain/core/messages';
 import type { BaseMessage } from '@langchain/core/messages';
 import { LangChainTracer } from '@langchain/core/tracers/tracer_langchain';
@@ -10,7 +11,8 @@ import { Client as TracingClient } from 'langsmith';
 import type { IUser, INodeTypeDescription, ITelemetryTrackProperties } from 'n8n-workflow';
 
 import { LLMServiceError } from '@/errors';
-import { anthropicClaudeSonnet45 } from '@/llm-config';
+import { anthropicClaudeSonnet45, MODEL_FACTORIES, getApiKeyEnvVar, type ModelId } from '@/llm-config';
+import { getProxyAgent } from '@/utils/http-proxy-agent';
 import { SessionManagerService } from '@/session-manager.service';
 import { ResourceLocatorCallbackFactory } from '@/types/callbacks';
 import {
@@ -74,6 +76,20 @@ export class AiWorkflowBuilderService {
 		});
 	}
 
+	// Generic factory to create a model by id using the registry in `llm-config`
+	private static async getModelById(
+		modelId: ModelId,
+		config: { baseUrl?: string; authHeaders?: Record<string, string>; apiKey?: string } = {},
+	): Promise<BaseChatModel> {
+		const factory = MODEL_FACTORIES[modelId];
+		if (!factory) {
+			throw new Error(`Unknown model id: ${modelId}`);
+		}
+
+		// pass through headers and apiKey according to factory expectations
+		return await factory({ apiKey: config.apiKey ?? '', baseUrl: config.baseUrl, headers: config.authHeaders });
+	}
+
 	private async getApiProxyAuthHeaders(user: IUser, userMessageId: string) {
 		assert(this.client);
 
@@ -90,18 +106,21 @@ export class AiWorkflowBuilderService {
 		user: IUser,
 		userMessageId: string,
 	): Promise<{
-		anthropicClaude: ChatAnthropic;
+		anthropicClaude: BaseChatModel;
 		tracingClient?: TracingClient;
 		// eslint-disable-next-line @typescript-eslint/naming-convention
 		authHeaders?: { Authorization: string };
 	}> {
 		try {
-			// If client is provided, use it for API proxy
-			if (this.client) {
+			// By default, prefer direct provider usage via environment variables.
+			// To enable proxy usage (old behaviour) set N8N_AI_USE_PROXY=true and ensure `this.client` is provided.
+			const useProxy = process.env.N8N_AI_USE_PROXY === 'true' && !!this.client;
+
+			if (useProxy) {
 				const authHeaders = await this.getApiProxyAuthHeaders(user, userMessageId);
 
 				// Extract baseUrl from client configuration
-				const baseUrl = this.client.getApiProxyBaseUrl();
+				const baseUrl = this.client!.getApiProxyBaseUrl();
 
 				const anthropicClaude = await AiWorkflowBuilderService.getAnthropicClaudeModel({
 					baseUrl: baseUrl + '/anthropic',
@@ -123,12 +142,84 @@ export class AiWorkflowBuilderService {
 				return { tracingClient, anthropicClaude, authHeaders };
 			}
 
-			// If base URL is not set, use environment variables
-			const anthropicClaude = await AiWorkflowBuilderService.getAnthropicClaudeModel({
-				apiKey: process.env.N8N_AI_ANTHROPIC_KEY ?? '',
+			// Otherwise use configured provider/model via environment variables.
+			// Support OpenAI-compatible providers (e.g., Deepseek) via:
+			// N8N_AI_PROVIDER=openai
+			// N8N_AI_ASSISTANT_BASE_URL=https://api.deepseek.com
+			// N8N_AI_MODEL_NAME=deepseek-chat
+			const providerEnv = (process.env.N8N_AI_PROVIDER ?? 'anthropic').toLowerCase();
+			const assistantBaseUrl = process.env.N8N_AI_ASSISTANT_BASE_URL;
+			const modelName = process.env.N8N_AI_MODEL_NAME;
+
+			if (providerEnv === 'openai' && assistantBaseUrl && modelName) {
+				// Use ChatOpenAI with custom base URL to call OpenAI-compatible providers directly
+				const { ChatOpenAI } = await import('@langchain/openai');
+
+				const apiKey = process.env.N8N_AI_OPENAI_KEY ?? '';
+
+				const openaiModel = new ChatOpenAI({
+					model: modelName,
+					apiKey,
+					temperature: 0,
+					maxTokens: -1,
+					configuration: {
+						baseURL: assistantBaseUrl,
+						defaultHeaders: {},
+						fetchOptions: {
+							dispatcher: getProxyAgent(assistantBaseUrl),
+						},
+					},
+				});
+
+				return { anthropicClaude: openaiModel };
+			}
+
+			if (providerEnv === 'anthropic' && assistantBaseUrl && modelName) {
+				// Use ChatAnthropic with custom base URL and model name to call Anthropic-compatible providers directly
+				const { ChatAnthropic: ChatAnthropicDynamic } = await import('@langchain/anthropic');
+
+				const apiKey = process.env.N8N_AI_ANTHROPIC_KEY ?? '';
+
+				const anthropicModel = new ChatAnthropicDynamic({
+					model: modelName,
+					apiKey,
+					temperature: 0,
+					// Keep other defaults; set api url to assistantBaseUrl
+					anthropicApiUrl: assistantBaseUrl,
+					clientOptions: {
+						defaultHeaders: {
+							// keep prompt-caching header consistent with existing factories
+							// eslint-disable-next-line @typescript-eslint/naming-convention
+							'anthropic-beta': 'prompt-caching-2024-07-31',
+						},
+						fetchOptions: {
+							dispatcher: getProxyAgent(assistantBaseUrl),
+						},
+					},
+				});
+
+				// Remove topP if present to avoid provider-specific conflicts
+				// eslint-disable-next-line @typescript-eslint/no-explicit-any
+				delete (anthropicModel as any).topP;
+
+				return { anthropicClaude: anthropicModel };
+			}
+
+			// Fallback: use MODEL_FACTORIES by model id if provided via N8N_AI_MODEL_ID
+			const configuredModel = (process.env.N8N_AI_MODEL_ID as ModelId) ?? ('claude-sonnet-4.5' as ModelId);
+			const apiKeyEnv = getApiKeyEnvVar(configuredModel);
+			const apiKey = process.env[apiKeyEnv] ?? '';
+
+			const model = await AiWorkflowBuilderService.getModelById(configuredModel, {
+				apiKey,
+				baseUrl: assistantBaseUrl,
+				authHeaders: assistantBaseUrl
+					? // pass empty headers object to allow factories to merge headers if needed
+						{}
+					: undefined,
 			});
 
-			return { anthropicClaude };
+			return { anthropicClaude: model };
 		} catch (error) {
 			const errorMessage = error instanceof Error ? `: ${error.message}` : '';
 			const llmError = new LLMServiceError(`Failed to connect to LLM Provider${errorMessage}`, {
